services:
  comfyui:
    build:
      context: .
      dockerfile: Dockerfile.comfyui
    container_name: freya-comfyui
    network_mode: host
    volumes:
      - ./comfyui/models:/app/ComfyUI/models
      - ./comfyui/output:/app/ComfyUI/output
      - ./comfyui/input:/app/ComfyUI/input
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped

  swarmui:
    build:
      context: .
      dockerfile: Dockerfile.swarmui
    container_name: freya-swarmui
    network_mode: host
    volumes:
      - ./swarmui/models:/app/SwarmUI/Data/Models
      - ./swarmui/output:/app/SwarmUI/Data/Outputs
      - ./swarmui/input:/app/SwarmUI/Data/Inputs
      - ./swarmui/data:/app/SwarmUI/Data
      - ./swarmui/dlbackend:/app/SwarmUI/dlbackend
      - ./swarmui/swarmui-models:/app/SwarmUI/Models
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: freya-ollama
    network_mode: host
    volumes:
      - ./ollama/models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped

  prompt-generator:
    build:
      context: ./prompt-generator
      dockerfile: Dockerfile
    container_name: freya-prompt-generator
    network_mode: host
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - OLLAMA_URL=http://localhost:11434
      - OLLAMA_MODEL=llama3.2:1b
      - USE_LLM=true
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: freya-ollama
    network_mode: host
    volumes:
      - ./ollama/models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped

  prompt-generator:
    build:
      context: ./prompt-generator
      dockerfile: Dockerfile
    container_name: freya-prompt-generator
    network_mode: host
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - OLLAMA_URL=http://localhost:11434
      - OLLAMA_MODEL=llama3.2:1b
      - USE_LLM=true
    depends_on:
      - ollama
    restart: unless-stopped
